{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUHQ07ASfRLHURbBVNbN4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnasAlhasan/large-models-course/blob/main/notebooks/InferenceForLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Techniques to Make Inference Efficient**\n",
        "**(A) Quantization**\n",
        "\n",
        "Store weights in lower precision (e.g., 8-bit instead of 16/32-bit).\n",
        "\n",
        "Example:\n",
        "\n",
        "Original weight = 123.4567 (float32)\n",
        "\n",
        "Quantized = 123 (int8)\n",
        "\n",
        "Trade-off: smaller, faster, but tiny accuracy loss.\n",
        "\n",
        "üëâ Colab Demo (quantization on a small Hugging Face model):"
      ],
      "metadata": {
        "id": "8RdiGCfjdixK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First install the required dependencies\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers accelerate"
      ],
      "metadata": {
        "id": "tqEJlI6DeOm9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Configure 8-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0\n",
        ")\n",
        "\n",
        "# Load model with error handling\n",
        "model_name = \"facebook/opt-350m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error loading quantized model: {e}\")\n",
        "    print(\"Falling back to FP16 precision...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "# Run inference\n",
        "inputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\").to(\"cuda\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=30)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNblS4naecv9",
        "outputId": "6ac90369-93e3-4971-a3a0-62b73f438d30"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading quantized model: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
            "Falling back to FP16 precision...\n",
            "The future of AI is in the hands of the AI community\n",
            "\n",
            "The future of AI is in the hands of the AI community\n",
            "\n",
            "The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(B) Pruning**\n",
        "\n",
        "Remove unnecessary neurons/weights (like trimming branches of a tree).\n",
        "\n",
        "Example: if a neuron‚Äôs output is always near zero ‚Üí cut it.\n",
        "\n",
        "Speeds up inference but needs fine-tuning after pruning."
      ],
      "metadata": {
        "id": "s9nfJF7ijJxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(C) Distillation**\n",
        "\n",
        "Train a smaller ‚Äústudent‚Äù model to mimic a large teacher model.\n",
        "\n",
        "Example: DistilBERT is a smaller version of BERT, almost same performance, but much faster."
      ],
      "metadata": {
        "id": "7P5W_FbwjNBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Hands-On: Prompt Engineering**\n",
        "\n",
        "Once the model runs, prompting affects quality more than size.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "cZtiKbB8jdeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Translate this English text to French: 'How are you today?'\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oif2V8_RjMPX",
        "outputId": "39288376-f199-4cff-f293-e3d8011bd63a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate this English text to French: 'How are you today?'\n",
            "\n",
            "The following is a translation of the text of the letter from the French Ministry of Foreign Affairs to the French Ambassador to the United States.\n",
            "\n",
            "Dear Ambassador,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Vn3vubijhxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}