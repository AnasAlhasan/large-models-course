{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPz+2rr62rbnRBvPknbATJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnasAlhasan/large-models-course/blob/main/notebooks/large_language_model_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Downloading the necessary libraries**"
      ],
      "metadata": {
        "id": "jnc1djLPzjgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkY82CYsstZb"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch torchvision torchaudio transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Selecting the Device**"
      ],
      "metadata": {
        "id": "ctseq7Wezeo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device =\"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "cGYtexQQtAfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = AutoTokenizer.from_pretrained(\"gpt2\") #converts the text to numbers so the model can understand\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device) #download the model and then send to the device that will generate the response"
      ],
      "metadata": {
        "id": "BaTtNzQqtjT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Testing the model**"
      ],
      "metadata": {
        "id": "G81ToJIyz5jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Artificial intelligence will\"\n",
        "inputs = token.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model.generate(inputs, max_length = 40, do_sample=True, top_k= 50)\n",
        "\n",
        "\n",
        "print(token.decode(outputs[0], skip_special_tokens= True))"
      ],
      "metadata": {
        "id": "JUS4_tJBuGcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Benchmarking the Model**"
      ],
      "metadata": {
        "id": "3IhG03DVzRbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = token.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "\n",
        "for batch in [1,2,4,8]:\n",
        "  batch_inputs = inputs.repeat(batch,1)\n",
        "  start = time.time()\n",
        "  with torch.no_grad():\n",
        "    _ = model.generate(batch_inputs, max_length=40, do_sample=True, top_k=50)\n",
        "    dur = time.time() - start\n",
        "    print(f\"Batch size: {batch} | Time: {dur:.4f}s\")"
      ],
      "metadata": {
        "id": "EgJ_G4nFutuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_gpt2():\n",
        "\n",
        "  while True:\n",
        "     user_prompt = input('\\nwrite \"quit\" to exit\\nYou: ')\n",
        "\n",
        "     if user_prompt.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "     inputs = token.encode(user_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "     with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length = len(inputs[0])+100,\n",
        "            do_sample =True,\n",
        "            top_k =50,\n",
        "            pad_token_id = token.eos_token_id\n",
        "        )\n",
        "\n",
        "     response = token.decode(outputs[0], skip_special_tokens =True)\n",
        "\n",
        "     generatedPart = response[len(user_prompt):]\n",
        "\n",
        "     print(\"gpt2: \", generatedPart.strip())\n",
        "\n"
      ],
      "metadata": {
        "id": "NjEh9hQlv0yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Testing the model**"
      ],
      "metadata": {
        "id": "hTnHinhr7GEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_gpt2()"
      ],
      "metadata": {
        "id": "8DmEvzWq2rry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Insights**\n",
        "This model is used to predict the next word when given a prompt, it highly unlikely to generate answers unrelated to the topic. It just predicts the next word. the **AutoTokenizer Library** is used to convert the prompt to numbers and then send it to the device. **AutoModelforCausalLM** is used for setting up the model and put it on device to run it."
      ],
      "metadata": {
        "id": "vuQ6oT8F7LV-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gNcEvBV68ZhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}