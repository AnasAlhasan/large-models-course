{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMwcKf7Bo0Sw/JGR0C4CFa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnasAlhasan/large-models-course/blob/main/notebooks/distributed_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3W63q10DahZy"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch torchvision torchaudio transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Activation Checkpointing**\n",
        "This techniques is all about **trading computation for memory**. When you're training a neural network, the program needs to store a lot of intermediate values (called \"activations\") during the forward pass so it can calculate the gradients during the backward pass. For very deep or wide models, these activations can eat up all your GPU memory.\n",
        "**Activate checkpointing** is a clever trick: it avoids storing some of these activations and simply recomputes them when they're needed during the backward pass."
      ],
      "metadata": {
        "id": "D2xPPPWFbA5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "Yyztl1QVenvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.checkpoint as checkpoint"
      ],
      "metadata": {
        "id": "tN0UrMYPapKz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feed-forward Block"
      ],
      "metadata": {
        "id": "J6ZaM6h3ezjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.Linear1 = nn.Linear(size, size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.Linear2 = nn.Linear(size,size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.Linear2(self.relu(self.Linear1(x)))\n",
        "\n",
        "\n",
        "class BlockwithCheckpoint(nn.Module):\n",
        "  def __init__(self,size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.block = Block(size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return checkpoint.checkpoint(self.block, x)\n",
        "\n"
      ],
      "metadata": {
        "id": "lsJGQaQGeyMo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing between two approaches"
      ],
      "metadata": {
        "id": "8wAEffBag8g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.randn(1,1000, requires_grad= True)\n",
        "x2 = x1.clone().detach().requires_grad_(True)\n",
        "model_normal = Block(1000)\n",
        "model_checkpoint = BlockwithCheckpoint(1000)\n",
        "\n",
        "out1 = model_normal(x1).sum()\n",
        "out2 = model_checkpoint(x2).sum()\n",
        "\n",
        "out1.backward()\n",
        "print(out1)\n",
        "out2.backward()\n",
        "print(out2)\n",
        "print(\"Both models ran successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCDl5e6SgxMk",
        "outputId": "26fbdb3d-170a-44fb-a488-d2e42ea6a27f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.4170, grad_fn=<SumBackward0>)\n",
            "tensor(-11.2639, grad_fn=<SumBackward0>)\n",
            "Both models ran successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Gradient Accumulation**\n",
        "his technique is a way to simulate a larger batch size than your GPU can physically handle. Training with larger batches can make the training process more stable. If a batch size of 1024 gives the best results but you can only fit a batch of 256 into memory, gradient accumulation is the solution.\n",
        "\n",
        "The idea is to process several small batches, calculate their gradients, and add them up (accumulate them). Only after you've processed enough small batches to equal your desired large batch size do you update the model's weights."
      ],
      "metadata": {
        "id": "izWVaIaUmg4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "rxfrOz2bmGDC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(10,1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "data = torch.randn(100, 10)\n",
        "targets = torch.randn(100, 1)\n",
        "\n",
        "batch_size = 20\n",
        "accum_steps = 4\n",
        "\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for i in range(0, len(data), batch_size):\n",
        "  x = data[i: i+batch_size]\n",
        "  y = targets[i: i+batch_size]\n",
        "\n",
        "  out = model(x)\n",
        "  loss = loss_fn(out,x)\n",
        "\n",
        "  loss= loss / accum_steps\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  if ( i // batch_size + 1) % accum_steps == 0:\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "print(\"Training with gradient accumulation complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bwQwtgamtqQ",
        "outputId": "18480f89-d460-4ffb-ad4c-56f6ec6b2f08"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "Training with gradient accumulation complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([20, 10])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Raq8gnmCoSKs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}