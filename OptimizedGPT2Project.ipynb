{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMY1f2KXUJPLaLqsNapG9jD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnasAlhasan/large-models-course/blob/main/OptimizedGPT2Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Fine-tune a tiny GPT model**"
      ],
      "metadata": {
        "id": "73KvJSnOL_lG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpsa-SyuKLhO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#core imports\n",
        "import os, time, math, torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "D0On3hSYK5Gw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Device Check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# show installed transformer version\n",
        "import transformers\n",
        "print(\"Transformers version:\", transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oRs2Bh6LMgy",
        "outputId": "2cc0b8fa-3484-427a-9531-30d87e0e52a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n",
            "Transformers version: 4.56.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ModelName = \"sshleifer/tiny-gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ModelName)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "Model = AutoModelForCausalLM.from_pretrained(ModelName)\n",
        "Model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzIm9MvBM0E3",
        "outputId": "1ed25ffb-83e3-4a4f-ab12-0412886c2005"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 2)\n",
              "    (wpe): Embedding(1024, 2)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-1): 2 x GPT2Block(\n",
              "        (ln_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=6, nx=2)\n",
              "          (c_proj): Conv1D(nf=2, nx=2)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=8, nx=2)\n",
              "          (c_proj): Conv1D(nf=2, nx=8)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making simple dataset\n",
        "texts = [\n",
        "    \"Artificial intelligence will change the world.\",\n",
        "    \"I am from Jordan and I live in irbid city.\",\n",
        "    \"Gym time is the best time in the day.\",\n",
        "    \"Technology will evolve using AI.\"\n",
        "] * 200\n",
        "\n",
        "print(\"Dataset size(number of lines): \", len(texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CAgh5HyNmwj",
        "outputId": "7fa970cf-36b9-4ccd-b8b8-2d52ec03ad4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size(number of lines):  800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40, 716, 422, 8078, 290, 314, 2107, 287, 4173, 14065, 1748, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a sample dataset\n",
        "class SimpleTextDataset(Dataset):\n",
        "  def __init__(self, texts, tokenizer, seq_len=64):\n",
        "    self.examples = []\n",
        "    for t in texts:\n",
        "      enc = tokenizer.encode(t, add_special_tokens=True)\n",
        "      for i in range(0, len(enc), seq_len):\n",
        "        chunk = enc[i:i+seq_len]\n",
        "        if len(chunk) < seq_len:\n",
        "          chunk = chunk + [tokenizer.pad_token_id] * (seq_len - len(chunk))\n",
        "        self.examples.append(torch.tensor(chunk, dtype=torch.long))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.examples[idx]\n",
        "\n",
        "SEQ_LENGTH = 64 # could be changed\n",
        "\n",
        "dataset = SimpleTextDataset(texts, tokenizer,SEQ_LENGTH)\n",
        "print(\"Number of training examples(chunks): \", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAxLweadOYEQ",
        "outputId": "3ca3db7e-bcdc-4ca9-d520-d73a92256355"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples(chunks):  800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "\n",
        "BATCH_SIZE = 4 #micro-batch size\n",
        "ACCUM_STEPS = 4 #gradient accumulation steps\n",
        "EPOCHS = 100\n",
        "LEARN_RATE = 5e-5\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(\"Dataloader is ready. Batches per epoch: \",math.ceil(len(dataset)/BATCH_SIZE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI32uXruQnJy",
        "outputId": "1736e851-8677-464f-8ad4-b235637d4627"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloader is ready. Batches per epoch:  200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Optimizing\n",
        "optimizer = torch.optim.AdamW(Model.parameters(), lr=LEARN_RATE)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "Model.train()\n",
        "global_step = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss =0.0\n",
        "  optimizer.zero_grad()\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    batch = batch.to(device)\n",
        "\n",
        "    with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "      outputs = Model(input_ids= batch, labels=batch)\n",
        "      loss = outputs.loss / ACCUM_STEPS\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    if (step + 1) % ACCUM_STEPS == 0:\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      optimizer.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "    epoch_loss += loss.item() * ACCUM_STEPS\n",
        "\n",
        "\n",
        "  avg_loss = epoch_loss / len(dataloader)\n",
        "  print(f\"Epoch {epoch+1} completed. Avg loss: {avg_loss: .4f} (global steps: {global_step})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuRv0MojSXOa",
        "outputId": "dab9f281-66db-4f1f-c2a5-0c965748562c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1296925172.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
            "/tmp/ipython-input-1296925172.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed. Avg loss:  10.6715 (global steps: 50)\n",
            "Epoch 2 completed. Avg loss:  10.6602 (global steps: 100)\n",
            "Epoch 3 completed. Avg loss:  10.6488 (global steps: 150)\n",
            "Epoch 4 completed. Avg loss:  10.6372 (global steps: 200)\n",
            "Epoch 5 completed. Avg loss:  10.6255 (global steps: 250)\n",
            "Epoch 6 completed. Avg loss:  10.6136 (global steps: 300)\n",
            "Epoch 7 completed. Avg loss:  10.6015 (global steps: 350)\n",
            "Epoch 8 completed. Avg loss:  10.5893 (global steps: 400)\n",
            "Epoch 9 completed. Avg loss:  10.5769 (global steps: 450)\n",
            "Epoch 10 completed. Avg loss:  10.5644 (global steps: 500)\n",
            "Epoch 11 completed. Avg loss:  10.5517 (global steps: 550)\n",
            "Epoch 12 completed. Avg loss:  10.5388 (global steps: 600)\n",
            "Epoch 13 completed. Avg loss:  10.5257 (global steps: 650)\n",
            "Epoch 14 completed. Avg loss:  10.5125 (global steps: 700)\n",
            "Epoch 15 completed. Avg loss:  10.4991 (global steps: 750)\n",
            "Epoch 16 completed. Avg loss:  10.4856 (global steps: 800)\n",
            "Epoch 17 completed. Avg loss:  10.4719 (global steps: 850)\n",
            "Epoch 18 completed. Avg loss:  10.4580 (global steps: 900)\n",
            "Epoch 19 completed. Avg loss:  10.4440 (global steps: 950)\n",
            "Epoch 20 completed. Avg loss:  10.4298 (global steps: 1000)\n",
            "Epoch 21 completed. Avg loss:  10.4154 (global steps: 1050)\n",
            "Epoch 22 completed. Avg loss:  10.4009 (global steps: 1100)\n",
            "Epoch 23 completed. Avg loss:  10.3862 (global steps: 1150)\n",
            "Epoch 24 completed. Avg loss:  10.3713 (global steps: 1200)\n",
            "Epoch 25 completed. Avg loss:  10.3563 (global steps: 1250)\n",
            "Epoch 26 completed. Avg loss:  10.3412 (global steps: 1300)\n",
            "Epoch 27 completed. Avg loss:  10.3258 (global steps: 1350)\n",
            "Epoch 28 completed. Avg loss:  10.3104 (global steps: 1400)\n",
            "Epoch 29 completed. Avg loss:  10.2947 (global steps: 1450)\n",
            "Epoch 30 completed. Avg loss:  10.2789 (global steps: 1500)\n",
            "Epoch 31 completed. Avg loss:  10.2630 (global steps: 1550)\n",
            "Epoch 32 completed. Avg loss:  10.2469 (global steps: 1600)\n",
            "Epoch 33 completed. Avg loss:  10.2307 (global steps: 1650)\n",
            "Epoch 34 completed. Avg loss:  10.2143 (global steps: 1700)\n",
            "Epoch 35 completed. Avg loss:  10.1977 (global steps: 1750)\n",
            "Epoch 36 completed. Avg loss:  10.1810 (global steps: 1800)\n",
            "Epoch 37 completed. Avg loss:  10.1642 (global steps: 1850)\n",
            "Epoch 38 completed. Avg loss:  10.1472 (global steps: 1900)\n",
            "Epoch 39 completed. Avg loss:  10.1301 (global steps: 1950)\n",
            "Epoch 40 completed. Avg loss:  10.1128 (global steps: 2000)\n",
            "Epoch 41 completed. Avg loss:  10.0954 (global steps: 2050)\n",
            "Epoch 42 completed. Avg loss:  10.0778 (global steps: 2100)\n",
            "Epoch 43 completed. Avg loss:  10.0601 (global steps: 2150)\n",
            "Epoch 44 completed. Avg loss:  10.0423 (global steps: 2200)\n",
            "Epoch 45 completed. Avg loss:  10.0243 (global steps: 2250)\n",
            "Epoch 46 completed. Avg loss:  10.0062 (global steps: 2300)\n",
            "Epoch 47 completed. Avg loss:  9.9880 (global steps: 2350)\n",
            "Epoch 48 completed. Avg loss:  9.9696 (global steps: 2400)\n",
            "Epoch 49 completed. Avg loss:  9.9510 (global steps: 2450)\n",
            "Epoch 50 completed. Avg loss:  9.9324 (global steps: 2500)\n",
            "Epoch 51 completed. Avg loss:  9.9136 (global steps: 2550)\n",
            "Epoch 52 completed. Avg loss:  9.8947 (global steps: 2600)\n",
            "Epoch 53 completed. Avg loss:  9.8756 (global steps: 2650)\n",
            "Epoch 54 completed. Avg loss:  9.8564 (global steps: 2700)\n",
            "Epoch 55 completed. Avg loss:  9.8371 (global steps: 2750)\n",
            "Epoch 56 completed. Avg loss:  9.8176 (global steps: 2800)\n",
            "Epoch 57 completed. Avg loss:  9.7981 (global steps: 2850)\n",
            "Epoch 58 completed. Avg loss:  9.7784 (global steps: 2900)\n",
            "Epoch 59 completed. Avg loss:  9.7585 (global steps: 2950)\n",
            "Epoch 60 completed. Avg loss:  9.7386 (global steps: 3000)\n",
            "Epoch 61 completed. Avg loss:  9.7185 (global steps: 3050)\n",
            "Epoch 62 completed. Avg loss:  9.6983 (global steps: 3100)\n",
            "Epoch 63 completed. Avg loss:  9.6779 (global steps: 3150)\n",
            "Epoch 64 completed. Avg loss:  9.6575 (global steps: 3200)\n",
            "Epoch 65 completed. Avg loss:  9.6369 (global steps: 3250)\n",
            "Epoch 66 completed. Avg loss:  9.6162 (global steps: 3300)\n",
            "Epoch 67 completed. Avg loss:  9.5954 (global steps: 3350)\n",
            "Epoch 68 completed. Avg loss:  9.5745 (global steps: 3400)\n",
            "Epoch 69 completed. Avg loss:  9.5534 (global steps: 3450)\n",
            "Epoch 70 completed. Avg loss:  9.5322 (global steps: 3500)\n",
            "Epoch 71 completed. Avg loss:  9.5109 (global steps: 3550)\n",
            "Epoch 72 completed. Avg loss:  9.4894 (global steps: 3600)\n",
            "Epoch 73 completed. Avg loss:  9.4679 (global steps: 3650)\n",
            "Epoch 74 completed. Avg loss:  9.4462 (global steps: 3700)\n",
            "Epoch 75 completed. Avg loss:  9.4244 (global steps: 3750)\n",
            "Epoch 76 completed. Avg loss:  9.4025 (global steps: 3800)\n",
            "Epoch 77 completed. Avg loss:  9.3805 (global steps: 3850)\n",
            "Epoch 78 completed. Avg loss:  9.3584 (global steps: 3900)\n",
            "Epoch 79 completed. Avg loss:  9.3362 (global steps: 3950)\n",
            "Epoch 80 completed. Avg loss:  9.3138 (global steps: 4000)\n",
            "Epoch 81 completed. Avg loss:  9.2917 (global steps: 4050)\n",
            "Epoch 82 completed. Avg loss:  9.2692 (global steps: 4100)\n",
            "Epoch 83 completed. Avg loss:  9.2464 (global steps: 4150)\n",
            "Epoch 84 completed. Avg loss:  9.2236 (global steps: 4200)\n",
            "Epoch 85 completed. Avg loss:  9.2007 (global steps: 4250)\n",
            "Epoch 86 completed. Avg loss:  9.1776 (global steps: 4300)\n",
            "Epoch 87 completed. Avg loss:  9.1545 (global steps: 4350)\n",
            "Epoch 88 completed. Avg loss:  9.1312 (global steps: 4400)\n",
            "Epoch 89 completed. Avg loss:  9.1078 (global steps: 4450)\n",
            "Epoch 90 completed. Avg loss:  9.0843 (global steps: 4500)\n",
            "Epoch 91 completed. Avg loss:  9.0607 (global steps: 4550)\n",
            "Epoch 92 completed. Avg loss:  9.0370 (global steps: 4600)\n",
            "Epoch 93 completed. Avg loss:  9.0132 (global steps: 4650)\n",
            "Epoch 94 completed. Avg loss:  8.9892 (global steps: 4700)\n",
            "Epoch 95 completed. Avg loss:  8.9652 (global steps: 4750)\n",
            "Epoch 96 completed. Avg loss:  8.9410 (global steps: 4800)\n",
            "Epoch 97 completed. Avg loss:  8.9168 (global steps: 4850)\n",
            "Epoch 98 completed. Avg loss:  8.8924 (global steps: 4900)\n",
            "Epoch 99 completed. Avg loss:  8.8679 (global steps: 4950)\n",
            "Epoch 100 completed. Avg loss:  8.8433 (global steps: 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving checkpoints\n",
        "OUT_DIR = \"/content/fintuned_tiny_gpt2\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "Model.save_pretrained(OUT_DIR)\n",
        "tokenizer.save_pretrained(OUT_DIR)\n",
        "print(\"Save checkpoint to: \", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHLpI5H7WoEo",
        "outputId": "c1c929bb-65d8-44ef-869d-07fbf1fd002b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint to:  /content/fintuned_tiny_gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference and Testing\n",
        "# Reload the model and weights\n",
        "tokenizer = AutoTokenizer.from_pretrained(OUT_DIR)\n",
        "Model = AutoModelForCausalLM.from_pretrained(OUT_DIR).to(device)\n",
        "Model.eval()\n",
        "\n",
        "prompt = \"Jordan\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "#Generate text\n",
        "\n",
        "with torch.no_grad():\n",
        "  generated = Model.generate(**inputs, max_length=80, do_sample=True, top_k=50)\n",
        "\n",
        "\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-7zG-ucU3hU",
        "outputId": "4d743518-9a9e-4d9c-b13f-cc676bf3737b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jordanificial am city Jordan ir factors in best I intelligence. using. intelligenceTechnology deflect Jordan change AI world dayArt ir mutual live city usingificial time in change from am\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Benchmarking\n",
        "import gc\n",
        "\n",
        "\n",
        "def time_generation(use_amp):\n",
        "  Model.to(device)\n",
        "  Model.eval()\n",
        "  start = time.time()\n",
        "  with torch.no_grad():\n",
        "    if use_amp and device.type == \"cuda\":\n",
        "      with torch.cuda.amp.autocast():\n",
        "        _ = Model.generate(**inputs, max_length=80)\n",
        "    else:\n",
        "      _ = Model.generate(**inputs, max_length=80)\n",
        "  return time.time() - start\n",
        "\n",
        "\n",
        "t_no_amp = time_generation(False)\n",
        "t_amp = time_generation(True)\n",
        "print(f\"Generation time - no amp: {t_no_amp: .3f}s || with amp: {t_amp: .3f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8_AxeJqYHTd",
        "outputId": "ff9f08af-e297-4092-bf52-b47cd8acc74f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/tmp/ipython-input-1025064624.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation time - no amp:  0.218s || with amp:  0.398s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dynamic Quantization (CPU) demonstration\n",
        "\n",
        "\n",
        "#load model on cpu\n",
        "model_cpu = AutoModelForCausalLM.from_pretrained(OUT_DIR).to(\"cpu\")\n",
        "model_cpu.eval()\n",
        "\n",
        "\n",
        "#apply dynamic quantization to linear layers\n",
        "\n",
        "model_q = torch.quantization.quantize_dynamic(model_cpu, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "#compare file sizes\n",
        "\n",
        "def dir_size(path):\n",
        "  total = 0\n",
        "  for dirpath, dirnames, filenames in os.walk(path):\n",
        "    for f in filenames:\n",
        "      fp = os.path.join(dirpath, f)\n",
        "      total += os.path.getsize(fp)\n",
        "  return total / (1024*1024)\n",
        "\n",
        "\n",
        "print(\"Original save model size (MB): \", dir_size(OUT_DIR))\n",
        "#Saving quantized model temporarily to measure size\n",
        "Q_DIR = \"/content/quantized_model\"\n",
        "os.makedirs(Q_DIR, exist_ok= True)\n",
        "torch.save(model_q.state_dict(), os.path.join(Q_DIR, \"pytorch_model_quantized.pt\"))\n",
        "print(\"Quantized model state dict saved (MB): \", dir_size(Q_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fadtoIEtZcQB",
        "outputId": "c0b0364f-f51b-4d72-d4dc-ed39233a9447"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original save model size (MB):  4.985299110412598\n",
            "Quantized model state dict saved (MB):  0.49916648864746094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1531951341.py:11: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_q = torch.quantization.quantize_dynamic(model_cpu, {torch.nn.Linear}, dtype=torch.qint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gx1e8XnJbj-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}